{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300ecf5b",
   "metadata": {},
   "source": [
    "Reinforce log vizualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf753065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"10_rl_corrector_None_stacked_mlp.csv\")\n",
    "\n",
    "\n",
    "sns.kdeplot(x=\"reward\", data=df, fill=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"10_rl_corrector_None_stacked_mlp.csv\")\n",
    "\n",
    "# Convert the reward_list strings into actual Python lists\n",
    "df['reward_list'] = df['reward_list'].apply(ast.literal_eval)\n",
    "\n",
    "# Pad all reward lists to the same length with NaNs\n",
    "max_len = max(len(r) for r in df['reward_list'])\n",
    "reward_matrix = np.array([\n",
    "    r + [np.nan] * (max_len - len(r))  # pad with NaNs for consistent shape\n",
    "    for r in df['reward_list']\n",
    "])\n",
    "\n",
    "# Compute mean and standard deviation across runs\n",
    "mean_rewards = np.nanmean(reward_matrix, axis=0)\n",
    "std_rewards = np.nanstd(reward_matrix, axis=0)\n",
    "\n",
    "# Plot mean reward and standard deviation band\n",
    "plt.figure(figsize=(12, 6))\n",
    "episodes = np.arange(max_len)\n",
    "plt.plot(episodes, mean_rewards, label='Mean Reward', color='blue')\n",
    "plt.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards,\n",
    "                 alpha=0.3, label='±1 Std Dev', color='blue')\n",
    "plt.title('Mean and Std Dev of Rewards per Episode Across Runs')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# Load and prepare the data\n",
    "df = pd.read_csv('10_rl_corrector_None_stacked_mlp_PPO.csv')\n",
    "df['reward_list'] = df['reward_list'].apply(ast.literal_eval)\n",
    "\n",
    "# Convert reward lists to long-form DataFrame\n",
    "records = []\n",
    "for run_id, rewards in enumerate(df['reward_list']):\n",
    "    for episode, reward in enumerate(rewards):\n",
    "        records.append({'Run': run_id, 'Episode': episode, 'Reward': reward})\n",
    "\n",
    "long_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_theme(style='whitegrid', palette='viridis', font_scale=1.2)\n",
    "\n",
    "# Plot with confidence interval (default: 95%)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=long_df, x='Episode', y='Reward', errorbar='sd', color='purple')  # Use ci='sd' for ±1 std deviation\n",
    "plt.title('Reward per Episode with ±1 Std Dev (Seaborn)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40098ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_label(file):\n",
    "    name = os.path.splitext(os.path.basename(file))[0]\n",
    "    name = re.sub(r'^10_', '', name)                         # Remove leading \"10_\"\n",
    "    name = name.replace('_', ' ')                            # Replace _ with space\n",
    "    name = name.replace('rl corrector', 'Reinforce')\n",
    "    name = name.replace('None', 'C0')\n",
    "             # Replace label\n",
    "    name = re.sub(r'\\b20(\\.0)?\\b', '', name)                 # Remove 20 or 20.0\n",
    "    name = re.sub(r'\\s+', ' ', name)                         # Remove extra whitespace\n",
    "    name = re.sub(r'\\b5(\\.0)?\\b', '', name)        \n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "# Set up Seaborn style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "# Find all CSV files in the current directory\n",
    "csv_files = glob.glob(\"*.csv\")\n",
    "print(\"csv_files\", csv_files)\n",
    "# Prepare data for box plot\n",
    "data = []\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        if 'reward' in df.columns:\n",
    "            label = clean_label(file)\n",
    "            for value in df['reward']:\n",
    "\n",
    "                data.append({'File': label, 'Reward': value})\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {file} due to error: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "plot_df = pd.DataFrame(data)\n",
    "\n",
    "# Create box plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.boxplot(data=plot_df, x='File', y='Reward', palette=\"viridis\")\n",
    "\n",
    "# Aesthetic tweaks\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Distribution of Final Rewards Across CSV Files\")\n",
    "plt.xlabel(\"corrector\")\n",
    "plt.ylabel(\"Final Reward\")\n",
    "plt.tight_layout()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Save and show\n",
    "# plt.savefig(\"plots/boxplot_all_runs.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '10_rl_corrector_None_stacked_mlp_PPO.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define a function to extract numerical values from the string representation of arrays\n",
    "def extract_first_values_from_string(s):\n",
    "    # Use regular expression to find all array representations in the string\n",
    "    array_strings = re.findall(r'array\\(\\s*\\[([-+]?\\d*\\.\\d+|\\d+)\\s*\\]', s)\n",
    "    # Extract the first numerical value from each array string\n",
    "    return [float(arr) for arr in array_strings]\n",
    "\n",
    "# Extract the first values from the 'reward_list' column using the refined method\n",
    "reward_lists = []\n",
    "for reward_list in data['reward_list']:\n",
    "    print(reward_list)\n",
    "    try:\n",
    "        # Extract the first numerical values from the string\n",
    "        values = extract_first_values_from_string(reward_list)\n",
    "        # first_reward_values.extend(values)\n",
    "        reward_lists.append(values)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Skip entries with unexpected formats\n",
    "        continue\n",
    "\n",
    "# Plot the first reward values extracted from the 'reward_list' column\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(reward_lists, marker='o', linestyle='-')\n",
    "plt.title('First Reward Values from Each Array in Reward List')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c8064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 15:59:34.136422: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-27 15:59:34.162145: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[32m2025-06-27 15:59:35.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclassic_rl.rl_corrector\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m985\u001b[0m - \u001b[1mgamma 0.99, clip_epsilon 0.2, gae_lambda 1, num_episodes 10, k_epochs 80, vf_loss_coef 0.5, action_std 0.6, decay_action_std_rate 0.05, min_action_std 0.1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Device set to : NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "============================================================================================\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmartinsaquet/.pyenv/versions/env_aiming/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=32 and n_envs=1)\n",
      "  warnings.warn(\n",
      "/home/jmartinsaquet/.pyenv/versions/env_aiming/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "/home/jmartinsaquet/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/envs/classic_control/continuous_mountain_car.py:156: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  velocity += force * self.power - 0.0025 * math.cos(3 * position)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m env = gym.make(\u001b[33m\"\u001b[39m\u001b[33mMountainCarContinuous-v0\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# default goal_velocity=0\u001b[39;00m\n\u001b[32m      6\u001b[39m corrector = rl_corrector.PPOSB3Corrector(env, \u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m rlist  = \u001b[43mcorrector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SmartDartCorrector/classic_rl/rl_corrector.py:1050\u001b[39m, in \u001b[36mPPOSB3Corrector.train\u001b[39m\u001b[34m(self, n_episodes)\u001b[39m\n\u001b[32m   1048\u001b[39m action = action.cpu().detach()\n\u001b[32m   1049\u001b[39m \u001b[38;5;66;03m# action = self.ppo_agent.select_action(state)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m nexstate, reward, done, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# saving reward and is_terminals\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# self.ppo_agent.buffer.rewards.append(reward)\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.ppo_agent.buffer.is_terminals.append(done)\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28mself\u001b[39m.ppo_agent.rollout_buffer.add(state, action, reward, done, value.detach(), log_prob.detach())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/core.py:322\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    319\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    320\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    321\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/envs/classic_control/continuous_mountain_car.py:156\u001b[39m, in \u001b[36mContinuous_MountainCarEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    153\u001b[39m velocity = \u001b[38;5;28mself\u001b[39m.state[\u001b[32m1\u001b[39m]\n\u001b[32m    154\u001b[39m force = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(action[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.min_action), \u001b[38;5;28mself\u001b[39m.max_action)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[43mvelocity\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0025\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m velocity > \u001b[38;5;28mself\u001b[39m.max_speed:\n\u001b[32m    158\u001b[39m     velocity = \u001b[38;5;28mself\u001b[39m.max_speed\n",
      "\u001b[31mTypeError\u001b[39m: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations."
     ]
    }
   ],
   "source": [
    "from classic_rl import rl_corrector\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")  # default goal_velocity=0\n",
    "\n",
    "corrector = rl_corrector.PPOSB3Corrector(env, None) \n",
    "rlist  = corrector.train(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c57d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-27 15:59:52.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclassic_rl.rl_corrector\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m985\u001b[0m - \u001b[1mgamma 0.99, clip_epsilon 0.2, gae_lambda 1, num_episodes 10, k_epochs 80, vf_loss_coef 0.5, action_std 0.6, decay_action_std_rate 0.05, min_action_std 0.1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmartinsaquet/.pyenv/versions/env_aiming/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=32 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m env = gym.make(\u001b[33m\"\u001b[39m\u001b[33mMountainCarContinuous-v0\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# default goal_velocity=0\u001b[39;00m\n\u001b[32m      6\u001b[39m corrector = rl_corrector.PPOSB3Corrector(env, \u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m rlist  = \u001b[43mcorrector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SmartDartCorrector/classic_rl/rl_corrector.py:1050\u001b[39m, in \u001b[36mPPOSB3Corrector.train\u001b[39m\u001b[34m(self, n_episodes)\u001b[39m\n\u001b[32m   1048\u001b[39m action = action.cpu().detach()\n\u001b[32m   1049\u001b[39m \u001b[38;5;66;03m# action = self.ppo_agent.select_action(state)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m nexstate, reward, done, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# saving reward and is_terminals\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# self.ppo_agent.buffer.rewards.append(reward)\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.ppo_agent.buffer.is_terminals.append(done)\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28mself\u001b[39m.ppo_agent.rollout_buffer.add(state, action, reward, done, value.detach(), log_prob.detach())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/core.py:322\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    319\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    320\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    321\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/env_aiming/lib/python3.11/site-packages/gymnasium/envs/classic_control/continuous_mountain_car.py:156\u001b[39m, in \u001b[36mContinuous_MountainCarEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    153\u001b[39m velocity = \u001b[38;5;28mself\u001b[39m.state[\u001b[32m1\u001b[39m]\n\u001b[32m    154\u001b[39m force = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(action[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.min_action), \u001b[38;5;28mself\u001b[39m.max_action)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[43mvelocity\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0025\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m velocity > \u001b[38;5;28mself\u001b[39m.max_speed:\n\u001b[32m    158\u001b[39m     velocity = \u001b[38;5;28mself\u001b[39m.max_speed\n",
      "\u001b[31mTypeError\u001b[39m: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations."
     ]
    }
   ],
   "source": [
    "from classic_rl import rl_corrector\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")  # default goal_velocity=0\n",
    "\n",
    "corrector = rl_corrector.PPOSB3Corrector(env, None) \n",
    "rlist  = corrector.train(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d17066",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2738a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | -53.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 1752     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -51.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1366        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007100623 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.0706     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 0.0799      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 999        |\n",
      "|    ep_rew_mean          | -49.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1280       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00778725 |\n",
      "|    clip_fraction        | 0.0274     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | -0.08      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.844      |\n",
      "|    value_loss           | 0.031      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -47.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1233        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008454585 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | -0.00692    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0119     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.771       |\n",
      "|    value_loss           | 0.0221      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -45.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1207        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008737182 |\n",
      "|    clip_fraction        | 0.0304      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | -0.00481    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    std                  | 0.701       |\n",
      "|    value_loss           | 0.0161      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -43.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1203        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010434041 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0382      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    std                  | 0.638       |\n",
      "|    value_loss           | 0.0119      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -41.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1212         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075945123 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | -0.0259      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00324      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    std                  | 0.581        |\n",
      "|    value_loss           | 0.0101       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -39.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1219        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008636177 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.825      |\n",
      "|    explained_variance   | 0.00267     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    std                  | 0.529       |\n",
      "|    value_loss           | 0.00977     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -37.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1204        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008928774 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.732      |\n",
      "|    explained_variance   | 0.00767     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    std                  | 0.483       |\n",
      "|    value_loss           | 0.00708     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -35.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1190         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071536847 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.644       |\n",
      "|    explained_variance   | -0.0184      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0318      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0179      |\n",
      "|    std                  | 0.443        |\n",
      "|    value_loss           | 0.00606      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -33.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1180         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076774633 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.558       |\n",
      "|    explained_variance   | -0.0211      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0192      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.018       |\n",
      "|    std                  | 0.407        |\n",
      "|    value_loss           | 0.00476      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -32.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1172        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006761611 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | 0.00233     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00945    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 0.00423     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -30.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1169        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007458627 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | -0.0474     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0343     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.345       |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -29.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1169         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068201832 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.311       |\n",
      "|    explained_variance   | 0.00276      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0137      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0163      |\n",
      "|    std                  | 0.319        |\n",
      "|    value_loss           | 0.0033       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 999        |\n",
      "|    ep_rew_mean          | -28        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1171       |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00660423 |\n",
      "|    clip_fraction        | 0.0239     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.231     |\n",
      "|    explained_variance   | 0.0132     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.294      |\n",
      "|    value_loss           | 0.00236    |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -26.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1169         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056209224 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.149       |\n",
      "|    explained_variance   | 0.00879      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0158      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    std                  | 0.271        |\n",
      "|    value_loss           | 0.00179      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -25.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1164        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004919324 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0751     |\n",
      "|    explained_variance   | -0.0203     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.252       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -24.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1158        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005192527 |\n",
      "|    clip_fraction        | 0.0174      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00304    |\n",
      "|    explained_variance   | 0.00399     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    std                  | 0.235       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -23.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1157         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052843895 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0685       |\n",
      "|    explained_variance   | -0.00678     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0326      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    std                  | 0.218        |\n",
      "|    value_loss           | 0.001        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -22         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1153        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005024289 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.146       |\n",
      "|    explained_variance   | -0.0204     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.203       |\n",
      "|    value_loss           | 0.000702    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -21.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1145         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046019414 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.217        |\n",
      "|    explained_variance   | 0.00863      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0347      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    std                  | 0.189        |\n",
      "|    value_loss           | 0.000589     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 999        |\n",
      "|    ep_rew_mean          | -20.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1135       |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00652585 |\n",
      "|    clip_fraction        | 0.0241     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.291      |\n",
      "|    explained_variance   | 0.000979   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0302    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.000492   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -19.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1127        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005752446 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.365       |\n",
      "|    explained_variance   | 0.0235      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00803    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.000378    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -18.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1119         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052457005 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.438        |\n",
      "|    explained_variance   | -0.0336      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0141      |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    std                  | 0.151        |\n",
      "|    value_loss           | 0.000354     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -18.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1113        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004173957 |\n",
      "|    clip_fraction        | 0.0188      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.506       |\n",
      "|    explained_variance   | 0.00239     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00812    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000269    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -17.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1107        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004437414 |\n",
      "|    clip_fraction        | 0.0151      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.571       |\n",
      "|    explained_variance   | -0.229      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00737    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.00023     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -17         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1101        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004318285 |\n",
      "|    clip_fraction        | 0.0246      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.64        |\n",
      "|    explained_variance   | -0.0132     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00942    |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.00016     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -16.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1095         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046440223 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.707        |\n",
      "|    explained_variance   | -0.0309      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0063      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    std                  | 0.116        |\n",
      "|    value_loss           | 0.000147     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -16          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1089         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050422503 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.778        |\n",
      "|    explained_variance   | -0.0161      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0198      |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    std                  | 0.108        |\n",
      "|    value_loss           | 0.00011      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -15.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1084         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043956107 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.846        |\n",
      "|    explained_variance   | 0.00112      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    std                  | 0.101        |\n",
      "|    value_loss           | 0.0001       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -15          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1077         |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048084506 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.913        |\n",
      "|    explained_variance   | -0.00973     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0155      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00999     |\n",
      "|    std                  | 0.0945       |\n",
      "|    value_loss           | 6.46e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -14.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1072         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048295707 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.976        |\n",
      "|    explained_variance   | -0.0129      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00946     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.0102      |\n",
      "|    std                  | 0.0884       |\n",
      "|    value_loss           | 5.49e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -14.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1071         |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057944264 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.05         |\n",
      "|    explained_variance   | -0.0528      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0153      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    std                  | 0.0823       |\n",
      "|    value_loss           | 8.25e-05     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 999        |\n",
      "|    ep_rew_mean          | -13.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1073       |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00519726 |\n",
      "|    clip_fraction        | 0.0277     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.11       |\n",
      "|    explained_variance   | 0.00189    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.0769     |\n",
      "|    value_loss           | 4.22e-05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -13.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1075        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003998886 |\n",
      "|    clip_fraction        | 0.0187      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.18        |\n",
      "|    explained_variance   | 0.000414    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00989    |\n",
      "|    std                  | 0.0723      |\n",
      "|    value_loss           | 2.12e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -13          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1076         |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049846414 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.25         |\n",
      "|    explained_variance   | -0.00857     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0256      |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    std                  | 0.0677       |\n",
      "|    value_loss           | 1.93e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1077        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003991874 |\n",
      "|    clip_fraction        | 0.0151      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.31        |\n",
      "|    explained_variance   | -0.000763   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00923    |\n",
      "|    std                  | 0.0637      |\n",
      "|    value_loss           | 1.71e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -12.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1077         |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040424056 |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.37         |\n",
      "|    explained_variance   | 0.00604      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0238      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    std                  | 0.0597       |\n",
      "|    value_loss           | 1.44e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -12.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1079         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047963327 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.44         |\n",
      "|    explained_variance   | -0.0101      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0146      |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.011       |\n",
      "|    std                  | 0.0557       |\n",
      "|    value_loss           | 8.43e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -11.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1080         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038405843 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.5          |\n",
      "|    explained_variance   | 0.0119       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0199      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00953     |\n",
      "|    std                  | 0.0525       |\n",
      "|    value_loss           | 9.32e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -11.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1080        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003230686 |\n",
      "|    clip_fraction        | 0.0114      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.56        |\n",
      "|    explained_variance   | -0.0531     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00693    |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    std                  | 0.0495      |\n",
      "|    value_loss           | 2.21e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -11.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1082        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004421751 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.62        |\n",
      "|    explained_variance   | -0.0182     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0329     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.0463      |\n",
      "|    value_loss           | 4.56e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -10.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1083        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005748427 |\n",
      "|    clip_fraction        | 0.0231      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.68        |\n",
      "|    explained_variance   | -0.0432     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00799    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00803    |\n",
      "|    std                  | 0.0439      |\n",
      "|    value_loss           | 3.2e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -10.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1085        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004252152 |\n",
      "|    clip_fraction        | 0.0237      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.73        |\n",
      "|    explained_variance   | -0.00203    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00945    |\n",
      "|    std                  | 0.0417      |\n",
      "|    value_loss           | 1.05e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -10.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1086        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004643369 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.79        |\n",
      "|    explained_variance   | -0.000228   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00968    |\n",
      "|    std                  | 0.0394      |\n",
      "|    value_loss           | 6.96e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -10.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1088         |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037507256 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.85         |\n",
      "|    explained_variance   | 0.0133       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0309      |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00919     |\n",
      "|    std                  | 0.0371       |\n",
      "|    value_loss           | 1.18e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -9.97        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1090         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029053087 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.9          |\n",
      "|    explained_variance   | -0.0557      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0152      |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00834     |\n",
      "|    std                  | 0.0352       |\n",
      "|    value_loss           | 1.2e-06      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -9.77        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1091         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041628676 |\n",
      "|    clip_fraction        | 0.0194       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.96         |\n",
      "|    explained_variance   | 0.00102      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    std                  | 0.0331       |\n",
      "|    value_loss           | 6e-06        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 999          |\n",
      "|    ep_rew_mean          | -9.58        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1091         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051478986 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.02         |\n",
      "|    explained_variance   | -0.035       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0355      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00996     |\n",
      "|    std                  | 0.0315       |\n",
      "|    value_loss           | 6.15e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7a13880be4d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3 as sb\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")  \n",
    "\n",
    "\n",
    "mopel = sb.PPO(\n",
    "    \"MlpPolicy\",env, verbose=2)\n",
    "\n",
    "mopel.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a5d58fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "state = torch.tensor([[0.0, 0.0]]).to(mopel.device)\n",
    "mopel.policy(state, deterministic=True)\n",
    "\n",
    "state.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_aiming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
